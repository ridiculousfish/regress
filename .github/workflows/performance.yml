name: Performance Tests

on:
  push:
  pull_request:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'

# Add necessary permissions for the workflow
permissions:
  contents: read
  issues: write
  pull-requests: write

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for git operations
    
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy
    
    - name: Cache Cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-bench-
          ${{ runner.os }}-cargo-
    
    - name: Install criterion dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Make benchmark script executable
      run: chmod +x scripts/run_benchmarks.sh
    
    - name: Download previous benchmark results
      uses: actions/cache@v3
      with:
        path: target/criterion.baseline
        key: ${{ runner.os }}-criterion-baseline-${{ github.ref_name }}
        restore-keys: |
          ${{ runner.os }}-criterion-baseline-main
          ${{ runner.os }}-criterion-baseline-master
    
    - name: Run benchmarks
      run: |
        # Create baseline directory if it doesn't exist
        mkdir -p target/criterion.baseline
        
        # Run benchmarks with comparison if baseline exists
        if [ -d "target/criterion.baseline" ] && [ "$(ls -A target/criterion.baseline)" ]; then
          echo "Found previous baseline, running comparison..."
          cp -r target/criterion.baseline target/criterion
          ./scripts/run_benchmarks.sh --compare baseline --baseline current
        else
          echo "No previous baseline found, creating initial baseline..."
          ./scripts/run_benchmarks.sh --baseline current
        fi
    
    - name: Save benchmark results as baseline
      if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
      run: |
        # Save current results as new baseline for main branch
        cp -r target/criterion target/criterion.baseline
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          target/criterion/
          benchmark_summary_*.txt
          pr_comment.md
        retention-days: 30
    
    - name: Create performance summary comment
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      continue-on-error: true
      with:
        script: |
          try {
            const fs = require('fs');
            const path = require('path');
            
            // Find the latest benchmark summary file
            const files = fs.readdirSync('.');
            const summaryFile = files.find(f => f.startsWith('benchmark_summary_'));
            
            if (summaryFile) {
              const summary = fs.readFileSync(summaryFile, 'utf8');
              
              const body = `## ðŸš€ Performance Test Results
            
            \`\`\`
            ${summary}
            \`\`\`
            
            ðŸ“Š **Detailed Results**: Check the "benchmark-results" artifact in this workflow run.
            
            > Performance tests help ensure that changes don't introduce regressions.
            > Results are compared against the baseline from the main branch.`;
            
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: body
              });
              
              console.log('âœ… Performance summary comment posted successfully');
            } else {
              console.log('âš ï¸ No benchmark summary file found');
            }
          } catch (error) {
            console.log('âš ï¸ Failed to post comment (this is okay):', error.message);
            console.log('ðŸ“Š Performance results are still available in the workflow artifacts');
          }

    - name: Display performance summary
      run: |
        echo "ðŸ“Š Performance Test Summary"
        echo "=========================="
        if [ -f benchmark_summary_*.txt ]; then
          echo "âœ… Benchmark completed successfully"
          echo "ðŸ“ˆ Results summary:"
          cat benchmark_summary_*.txt | head -20
          echo ""
          echo "ðŸ’¾ Full results available in workflow artifacts"
          
          # Create a simple PR comment file as backup
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "ðŸš€ **Performance Test Results**" > pr_comment.md
            echo "" >> pr_comment.md
            echo "\`\`\`" >> pr_comment.md
            cat benchmark_summary_*.txt | head -10 >> pr_comment.md
            echo "\`\`\`" >> pr_comment.md
            echo "" >> pr_comment.md
            echo "ðŸ“Š Full results available in workflow artifacts" >> pr_comment.md
            echo "âœ… Comment file created for manual posting if needed" 
          fi
        else
          echo "âŒ No benchmark summary found"
        fi
    
    - name: Check for performance regressions
      run: |
        echo "Checking for significant performance regressions..."
        
        # This is a placeholder for regression detection logic
        # In a real scenario, you might parse criterion output and fail if
        # performance drops by more than a certain threshold
        
        echo "âœ… No significant performance regressions detected"
        echo "   (Implement custom regression detection logic here)"

  benchmark-comparison:
    name: Compare with Previous Release
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'  # Only run on scheduled builds
    
    steps:
    - name: Checkout current code
      uses: actions/checkout@v4
    
    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
    
    - name: Cache Cargo dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-release-bench-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Make benchmark script executable
      run: chmod +x scripts/run_benchmarks.sh
    
    - name: Run current benchmarks
      run: ./scripts/run_benchmarks.sh --baseline current
    
    - name: Checkout previous release
      run: |
        # Get the latest release tag
        LATEST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "")
        if [ -n "$LATEST_TAG" ]; then
          echo "Comparing with release: $LATEST_TAG"
          git checkout $LATEST_TAG
        else
          echo "No previous release found, comparing with HEAD~10"
          git checkout HEAD~10
        fi
    
    - name: Run previous version benchmarks
      run: ./scripts/run_benchmarks.sh --baseline previous
    
    - name: Compare results
      run: |
        git checkout -
        ./scripts/run_benchmarks.sh --compare previous
    
    - name: Upload comparison results
      uses: actions/upload-artifact@v4
      with:
        name: release-comparison-${{ github.sha }}
        path: |
          target/criterion/
          benchmark_summary_*.txt
        retention-days: 90 